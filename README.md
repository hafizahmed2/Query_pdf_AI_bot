## Overview:
This is AI chatbot. You can upload your pdfs and ask question related to the the uploaded file. Provided files text will be extracted and after chuking it will be saved in vector db. Based on user query most relevant context will be extracted and send to LLM to get response. Chaning mechanism is also implemented along with memory retention
## Tools Used

Following tools are utilized:
GPT-3.5: For LLM gpt-3.5-turbo is used
Langchain: It is used for main functionality, text splitter, embedding, vector store, retriever, memory and chains
PyPDF2: To extract the text from pdf
Streamlit: To create the front end of application. User can upload files and ask questions

## How to run the bot!
1. Install the packages specified in requirement.txt file
2. Add ```.env file``` to your root directory. Add ```openai_api_key``` variable and set its values to your open ai key
3. Run the streamlit application with ```streamlit run AIChatbot.py```
4. Application will be available at port 8501 on localhost.

## Working details
Chatbot flow
 PDF uploaded -> pdf/pdf's text extraction -> chunking -> word embeddings -> embeddings in vectore store -> User question -> Retrieve context from vector store (using retriever) -> send query and context to LLM -> LLM response to frontend

## How hallucination is avoided
1. RAG pipeline is implemented to ensure that only relevant context is sent to the LLM, reducing the chances of generating irrelevant or hallucinated responses.

2. Temperature value is set to 0, minimizing randomness in the responses generated by the LLM. Lower temperature values lead to more deterministic outputs, decreasing the likelihood of hallucination.

3. Chaining is utilized to maintain coherence and continuity in the conversation flow. By chaining responses based on the previous context, the chatbot reduces the probability of generating disconnected or hallucinated replies.

4. Memory is utilized to store the context of the conversation. The chatbot maintains consistency and coherence, minimizing the occurrence of hallucinated content.

5. Employed human-in-the-loop validation - Tested on multiple pdfs with multiple queries, no hallucination is observed!


## Testing Procedure
I provided the document related to healthcare. Following queries are asked
1. Summarize the document
2. What this document is about?
3. Provide key points from the document
Results were as expected. No hallucination issue is face

Then series of queries which are irrelevant
1. What is the capital of France?
Ans: Can't assist with this question
2. What transportation details are provided in document?
Ans: Can't provide answer for this
So if any irrelevant question is asked, which is not in context with provided pdf. It doesn't hallucinate, rather provides the expected response

Chatbot was asked many more question and same behavior was observed. So no hallucination problem is observed

## Evaluation
1. Relevance and Accuracy:
The chatbot's ability to summarize the document, provide key points, and offer an overview of the document indicates that it can effectively extract relevant information.
Handling irrelevant queries by gracefully stating its inability to assist without hallucinating demonstrates robustness in maintaining context and relevance.

2. Response Time:
Response time is very descent for both relevant and irrelevant queries response. But it should be noted that response time is majorly dependant on underline machine.

3. Scalability:
Tested the chatbot's performance with larger documents or a collection of documents. Still didn't face any hallucination problem. Both relevant and irrelevant queries are addressed properly

## Conclusion
1. Given more time, overall model accuracy and performance can be increased
2. System with GPU support can enhance the responce time
3. Given more time, more detailed testing of chatbot could be done. A/B testing could be performed in more detail
